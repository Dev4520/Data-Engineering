
## Scenario Based questions:
 Q1. Will the reducer work or not if you use “Limit 1” in any HiveQL query?
 Ans:- Yes reducer will work on "Limit 1" because limit is used for constrian the data with select method.
 
 Q2. Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients
      trying to access Hive at the same time?
Ans:- Default metastore configuration allow one client Hive excess to be open at time, if multiple clinet trying to axcess then you will get error from meatstore.

Q3. Suppose, I create a table that contains details of all the transactions done by the customers: 
    CREATE TABLE transaction_details 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    ) 
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ‘,’;
    Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month.
    But, Hive is taking too much time in processing this query. 
    How will you solve this problem and list the steps that I will be taking in order to do so?
    
Ans:- we will create orc table with same schema of transaction_detail table
     CREATE TABLE transaction_details_orc 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    )
    stored as orc;
    
    orc is will push-down,compression and imporeve the performance of the query
    - use <database name>;
    - set hive.cli.print.header=true;
    - selecty month, sum(amount) as total_revenue from transaction_table_orc;

Q4. How can you add a new partition for the month December in the above partitioned table?

Ans:- From static partion december month in partition table.
    CREATE TABLE transaction_details_static_part 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    )
    partiontioned by (december FLOAT);
    Data will be copied from transaction_details_orc table to transaction_details_static_part.
    After loading data table can be see with data in detail-
    - describe formatted transaction_details_static_part limit 10;
    For static propeties is-
    - set hive.mpred.mode = strict;
    
    
Q5.  I am inserting data into a table based on partitions dynamically. But, I received an error – 
      FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
      
Ans:- Before partitions dynamically one propertis shouled be used in hive-
      - set hive.exec.dynamic.partition.mode=nonstrict;


Q6. Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
    id first_name last_name email gender ip_address
    How will you consume this CSV file into the Hive warehouse using built-in SerDe?
    
Ans:- create data base;
    - create database sample;
    # create a table of csv Serde-
    - create table sample_csv
    (
    id int,
    first_name string,
    last_name string,
    email string,
    gender string,
    ip_address int
    )
    row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'                                                                           
    with serdeproperties (                                                                                                                  
    "separatorChar" = ",",                                                                                                                 
    "quoteChar" = "\"",                                                                                                                    
    "escapeChar" = "\\"                                                                                                                    
    )                                                                                                                                       
    stored as textfile                                                                                                                      
    tblproperties ("skip.header.line.count" = "1");
    
   # load dataset-
   - load data local inpath 'file:///temp/hive/sample/sample.csv' into table sample_csv;
   # add jar file into your hive shell
   - add jar /tmp/hive_class/hive-hcatalog-core-0.14.0.jar;
   # For table detail:
   - select # from sample_csv;
   # Now data can be see in hive ware house-
   hadoop fs -ls /use/hive/warehouse/sample.db/sample_csv
   
 Q7. Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
     The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
     So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
   
   
Ans:- First ctrate table in hive-
       create table small_csv
      (
      id int, 
      name string, 
      e-mail string, 
      country string
      )
      row format delimited
      fields terminated by ','
      tblproperties("skip.header.line.count"="1")  # This propertis will skip the csv file header in hive table only data will be loaded from HDSF to hive table
      # Then all small csv data can be loaded one by one after path location-
      - load data inpath 'file loacation of csv data' into table small_csv;
           .
           .
           .
           .
        n no of file data can be loaded in by this way in table
        
        # From another way also csv file can be loaded from creating external table-
        - create table small_external_csv
         (
      id int, 
      name string, 
      e-mail string, 
      country string
      )
        > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data_class_2/';  # here multiple file location can be give in one table for data loading.
        
       
       
 Q8. LOAD DATA LOCAL INPATH ‘Home/country/state/’
     OVERWRITE INTO TABLE address;
    The following statement failed to execute. What can be the cause?
    
Ans:- load data local inpath 'file:///home/country/state' into table table_nane;

Q9. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans:- Yes it is posible to add 100 noeds if there is already 100 nodes in hive because becaues of master and and slabe, which is control by master and storge is done
     by data node and controing is done by master.
     
     
Q10. Hive Practical questions:

     Hive Join operations

     Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
     Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
     )

     Now perform different joins operations on top of these tables
     (Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

     BUILD A DATA PIPELINE WITH HIVE

     Download a data from the given location - 
     https://archive.ics.uci.edu/ml/machine-learning-databases/00360/
     
     1. Create a hive table as per given schema in your dataset 
     2. try to place a data into table location
     3. Perform a select operation . 
     4. Fetch the result of the select operation in your local as a csv file . 
     5. Perform group by operation . 
     7. Perform filter operation at least 5 kinds of filter examples . 
     8. show and example of regex operation
     9. alter table operation 
     10. drop table operation
     12. order by operation . 
     13. where clause operations you have to perform . 
     14. sorting operation you have to perform . 
     15. distinct operation you have to perform . 
     16. like an operation you have to perform . 
     17. union operation you have to perform . 
     18. table view operation you have to perform . 

 Ans:-
      # create new data base-
      - create database hive_challenge_task3;
      # create table-
      - Create a  table CUSTOMERS
      (
      ID int,
      NAME string,
      AGE int,
      ADDRESS string,
      SALARY int
      )
      row format delimited
      fields terminated by ',';
      
      # create second table 
      - Create a Second  table ORDER
      (
      OID int,
      DATE int,
      CUSTOMER_ID int,
      AMOUNT int
      )
      row format delimited
      fields terminated by ',';
