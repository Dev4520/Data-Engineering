
## Scenario Based questions:
 Q1. Will the reducer work or not if you use “Limit 1” in any HiveQL query?
 Ans:- Yes reducer will work on "Limit 1" because limit is used for constrian the data with select method.
 
 Q2. Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients
      trying to access Hive at the same time?
Ans:- Default metastore configuration allow one client Hive excess to be open at time, if multiple clinet trying to axcess then you will get error from meatstore.

Q3. Suppose, I create a table that contains details of all the transactions done by the customers: 
    CREATE TABLE transaction_details 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    ) 
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ‘,’;
    Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month.
    But, Hive is taking too much time in processing this query. 
    How will you solve this problem and list the steps that I will be taking in order to do so?
    
Ans:- we will create orc table with same schema of transaction_detail table
     CREATE TABLE transaction_details_orc 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    )
    stored as orc;
    
    orc is will push-down,compression and imporeve the performance of the query
    
    
    
    
    
    
    
    
    
    - use <database name>;
    - set hive.cli.print.header=true;
    - selecty month, sum(amount) as total_revenue from transaction_table_orc;

Q4. How can you add a new partition for the month December in the above partitioned table?

Ans:- From static partion december month in partition table.
    CREATE TABLE transaction_details_static_part 
    (
    cust_id INT,
    amount FLOAT,
    month STRING,
    country STRING
    )
    partiontioned by (december FLOAT);
    Data will be copied from transaction_details_orc table to transaction_details_static_part.
    After loading data table can be see with data in detail-
    - describe formatted transaction_details_static_part limit 10;
    For static propeties is-
    - set hive.mpred.mode = strict;
    
    
Q5.  I am inserting data into a table based on partitions dynamically. But, I received an error – 
      FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
      
Ans:- Before partitions dynamically one propertis shouled be used in hive-
      - set hive.exec.dynamic.partition.mode=nonstrict;


Q6. Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
    id first_name last_name email gender ip_address
    How will you consume this CSV file into the Hive warehouse using built-in SerDe?
    
Ans:- create data base;
    - create database sample;
    # create a table of csv Serde-
    - create table sample_csv
    (
    id int,
    first_name string,
    last_name string,
    email string,
    gender string,
    ip_address int
    )
    row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'                                                                           
    with serdeproperties (                                                                                                                  
    "separatorChar" = ",",                                                                                                                 
    "quoteChar" = "\"",                                                                                                                    
    "escapeChar" = "\\"                                                                                                                    
    )                                                                                                                                       
    stored as textfile                                                                                                                      
    tblproperties ("skip.header.line.count" = "1");
    
   # load dataset-
   - load data local inpath 'file:///temp/hive/sample/sample.csv' into table sample_csv;
   # add jar file into your hive shell
   - add jar /tmp/hive_class/hive-hcatalog-core-0.14.0.jar;
   # For table detail:
   - select # from sample_csv;
   # Now data can be see in hive ware house-
   hadoop fs -ls /use/hive/warehouse/sample.db/sample_csv
   
 Q7. Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
     The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
     So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
   
   
Ans:- First ctrate table in hive-
       create table small_csv
      (
      id int, 
      name string, 
      e-mail string, 
      country string
      )
      row format delimited
      fields terminated by ','
      tblproperties("skip.header.line.count"="1")  # This propertis will skip the csv file header in hive table only data will be loaded from HDSF to hive table
      # Then all small csv data can be loaded one by one after path location-
      - load data inpath 'file loacation of csv data' into table small_csv;
           .
           .
           .
           .
        n no of file data can be loaded in by this way in table
        
        # From another way also csv file can be loaded from creating external table-
        - create table small_external_csv
         (
      id int, 
      name string, 
      e-mail string, 
      country string
      )
        > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data_class_2/';  # here multiple file location can be give in one table for data loading.
        
       
       
 Q8. LOAD DATA LOCAL INPATH ‘Home/country/state/’
     OVERWRITE INTO TABLE address;
    The following statement failed to execute. What can be the cause?
    
Ans:- load data local inpath 'file:///home/country/state' into table table_nane;

Q9. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans:- Yes it is posible to add 100 noeds if there is already 100 nodes in hive because becaues of master and and slabe, which is control by master and storge is done
     by data node and controing is done by master.
     
     
Q10. Hive Practical questions:   Hive Join operations
     Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
     Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
     )
     Now perform different joins operations on top of these tables
     (Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

 Ans:- # create csv data file in local-
       - vim customer.csv
       - cat customer.csv
       1,Devanshu,23,Bihar,10000
       2,Neeraj,30,Kolkata,20000
       3,Sanjeev,28,MP,50000
       4,Chandan,36,Karnatka,80000
       5,Shivam,25,Delhi,15000
       6,Sanjay,26,Maharashtra,10000
       
       # load csv local to HDFS location
       :- /shekhar/customer.csv
       
      # create new data base in hive-
      - create database hive_challenge_task3;
      - use hive_challenge_task3;
      
      # create table-
      - Create a  table CUSTOMERS
      (
      ID int,
      NAME string,
      AGE int,
      ADDRESS string,
      SALARY int
      )
      row format delimited
      fields terminated by ',';
      
      # load data to hive table-
      - load data local inpath 'file:///home/cloudera/shekahr/customer.csv' into table customer_csv;
      
      # To print data in table-
      - set hive.cli.print.header=true;
      
      # To see schema datatype-
      - describe customer_csv;
      :- col_name	      data_type	comment
           id                  	int                 	                    
           name                	string              	                    
           age                 	int                 	                    
           addres              	string              	                    
           salary              	int
      
      # To see table schema detail-
      - describe formatted customer_csv;
      :- hdfs://quickstart.cloudera:8020/user/hive/warehouse/hive_challenge_task3.db/customer_csv	
      
      # To see table metadata detail-
      - describe extended customer_csv;
      :- Detailed Table Information	Table(tableName:customer_csv, dbName:hive_challenge_task3, owner:cloudera, createTime:1663683701, lastAccessTime:0, retention:0, 
      sd:StorageDescriptor(cols:[FieldSchema(name:id, type:int, comment:null), FieldSchema(name:name, type:string, comment:null), FieldSchema(name:age, type:int,
      comment:null), FieldSchema(name:addres, type:string, comment:null), FieldSchema(name:salary, type:int, comment:null)], 
      location:hdfs://quickstart.cloudera:8020/user/hive/warehouse/hive_challenge_task3.db/customer_csv, inputFormat:org.apache.hadoop.mapred.TextInputFormat, 
      outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, 
      serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), bucketCols:[], sortCols:[], 
      parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], 
      parameters:{numFiles=1, last_modified_by=cloudera, last_modified_time=1663683866, transient_lastDdlTime=1663686826, COLUMN_STATS_ACCURATE=true, totalSize=156,
      numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
      
      # to see table data-
      - select * from custoemr_csv;
      customer_csv.id	customer_csv.name	customer_csv.age	customer_csv.addres	customer_csv.salary
      :- 1	             Devanshu	           23	           Bihar	                   10000
         2	             Neeraj	             30	           Kolkata	                 20000
         3	             Sanjeev	            28	           MP	                      50000
         4	             Chandan	            36	           Karnatka	                80000
         5	             Shivam	             25	           Delhi	                   15000
         6	             Sanjay	             26	           Maharashtra	             10000

      # Make order.csv file in local-
      - vim order.csv
      
      # to see csv file-
      - cat order_csv
      :- 101,2021-03-15,1,133
         102,2013-08-22,2,155
         103,2023-02-13,3,500
         104,2022-03-26,4,800
         105,2019-07-18,7,900
         106,2015-11-09,8,110
      
      # to use database-
      - use hive_challenge_task3;
      
      # to print data in table
      - set hive.print.header=true;
      
      ## create second table 
      - Create a Second  table ORDER
      (
      OID int,
      DATE timestamp,
      CUSTOMER_ID int,
      AMOUNT int
      )
      row format delimited
      fields terminated by ',';
      
       # load data to hive table-
      - load data local inpath 'file:///home/cloudera/shekahr/customer.csv' into table order_csv;
      
      # to print data in table-
      - set hive.cli.print.header=true;
      
      # to see table schema datatype-
      - describe order_csv;
      :- col_name	        data_type	       comment
         oid                 	int                 	                    
         date                	timestamp              	                    
         customer_id        	int                 	                    
         salary              	int
      
      # To see table schema detail-
      - describe formatted order_csv;
      :- hdfs://quickstart.cloudera:8020/user/hive/warehouse/hive_challenge_task3.db/order_csv
      
      # To see table metadata detail-
      - describe extended customer_csv;
      :- Detailed Table Information	Table(tableName:order_csv, dbName:hive_challenge_task3, owner:cloudera, createTime:1663739314, lastAccessTime:0, retention:0, 
      sd:StorageDescriptor(cols:[FieldSchema(name:oid, type:int, comment:null), FieldSchema(name:date, type:string, comment:null), FieldSchema(name:cunstomer_id, 
      type:int, comment:null), FieldSchema(name:salary, type:int, comment:null)], location:hdfs://quickstart.cloudera:8020/user/hive/warehouse/hive_challenge_task3.db/order_csv, 
      inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, 
      serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,}), 
      bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false),
      partitionKeys:[], parameters:{numFiles=1, last_modified_by=cloudera, last_modified_time=1663744784, transient_lastDdlTime=1663744784, COLUMN_STATS_ACCURATE=true,
      totalSize=126, numRows=0, rawDataSize=0}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE)
      
      # See data with schema-
      - select * from order_csv;
      :-order_csv.oid	order_csv.date	order_csv.cunstomer_id	order_csv.salary
         101	           2021-03-15	     1	                     133
         102	           2013-08-22	     2	                     155
         103	           2023-02-13	     3	                     500
         104	           2022-03-26	     4	                     800
         105	           2019-07-18	     7	                     900
         106	           2015-11-09	     8	                     110

      # Inner joint-
      # inner joint properties-
      - SET hive.auto.convert.join=false;
      
      # inner join of two tables-
      - select customer_csv.*, order_csv.*
       > from customer_csv join order_csv
       > on (customer_csv.id = order_csv.customer_id);
       
       :- customer_csv.id	customer_csv.name	customer_csv.age	customer_csv.addres	customer_csv.salary	order_csv.oid	order_csv.date	order_csv.customer_id	order_csv.salary
           1	Devanshu	23	Bihar	   10000	 101	 2021-03-15	 1	 133
           2	Neeraj	  30	Kolkata	 20000	 102	 2013-08-22	 2	 155
           3	Sanjeev	 28	MP	      50000	 103	 2023-02-13	 3	 500
           4	Chandan	 36	Karnatka	80000	 104	 2022-03-26	 4	 800
           
         # Left outer join-
         -  select customer_csv.*, order_csv.*
           > from customer_csv left outer join order_csv
           > on (customer_csv.id = order_csv.customer_id);
           
           :- customer_csv.id	customer_csv.name	customer_csv.age	customer_csv.addres	customer_csv.salary	order_csv.oid	order_csv.date	order_csv.customer_id	order_csv.salary
                1	Devanshu	23	Bihar	      10000	101	2021-03-15	  1	   133
                2	Neeraj	  30	Kolkata	    20000	102	2013-08-22	  2	   155
                3	Sanjeev	 28	MP	         50000	103	2023-02-13	  3	   500
                4	Chandan	 36	Karnatka	   80000	104	2022-03-26	  4	   800
                5	Shivam	  25	Delhi	      15000	NULL	  NULL	    NULL	 NULL
                6	Sanjay	  26	Maharashtra	10000	NULL	  NULL	    NULL	 NULL
                
                
            # Right outer join-
            - select customer_csv.*, order_csv.*
            > from customer_csv right outer join order_csv
            > on (customer_csv.id = order_csv.customer_id);
            
            :- customer_csv.id	customer_csv.name	customer_csv.age	customer_csv.addres	customer_csv.salary	order_csv.oid	order_csv.date	order_csv.customer_id	order_csv.salary
                1	Devanshu	23	Bihar	   10000	101	2021-03-15	1	133
                2	Neeraj	  30	Kolkata	 20000	102	2013-08-22	2	155
                3	Sanjeev	 28	MP	      50000	103	2023-02-13	3	500
                4	Chandan	 36	Karnatka	80000	104	2022-03-26	4	800
                NULL	NULL	NULL	NULL	   NULL	 105	2019-07-18	7	900
                NULL	NULL	NULL	NULL	   NULL	 106	2015-11-09	8	110
                
             # Full Outer join-
             - select customer_csv.*, order_csv.*
             > from customer_csv full outer join order_csv
             > on (customer_csv.id = order_csv.cudtomer_id);
             
             :- customer_csv.id	customer_csv.name	customer_csv.age	customer_csv.addres	customer_csv.salary	order_csv.oid	order_csv.date	order_csv.customer_id	order_csv.salary
                  1	Devanshu	23	Bihar	      10000	101	2021-03-15	 1	  133
                  2	Neeraj	  30	Kolkata	    20000	102	2013-08-22	 2	  155
                  3	Sanjeev	 28	MP	         50000	103	2023-02-13	 3	  500
                  4	Chandan	 36	Karnatka	   80000	104	2022-03-26	 4	  800
                  5	Shivam	  25	Delhi	      15000	NULL	  NULL	   NULL	NULL
                  6	Sanjay	  26	Maharashtra	10000	NULL	  NULL	   NULL	NULL
                  NULL	NULL	NULL	  NULL	     NULL	105	2019-07-18	 7	  900
                  NULL	NULL	NULL	  NULL	     NULL	106	2015-11-09	 8	  110

 
 Q11. BUILD A DATA PIPELINE WITH HIVE

        Download a data from the given location - 
        https://archive.ics.uci.edu/ml/machine-learning-databases/00360/
        - data set downloaded and converted in csv file.
        
        # Attribute Information of data set:-
           0 Date (DD/MM/YYYY)
           1 Time (HH.MM.SS)
           2 True hourly averaged concentration CO in mg/m^3 (reference analyzer)
           3 PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)
           4 True hourly averaged overall Non Metanic HydroCarbons concentration in microg/m^3 (reference analyzer)
           5 True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)
           6 PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)
           7 True hourly averaged NOx concentration in ppb (reference analyzer)
           8 PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)
           9 True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)
           10 PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)
           11 PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)
           12 Temperature in Â°C
           13 Relative Humidity (%)
           14 AH Absolute Humidity
           
 1. Create a hive table as per given schema in your dataset 
        
        # csv file copy to HDFS loaction-
        :- -rw-r--r--   1 cloudera supergroup     762451 2022-09-21 04:31 /shekhar/AirQualityUCI.csv
         
         # created a table in hive-
         - create table airqualityuci_csv
         (
         Date string,
         Time string,
         CO float,
         PT08_S1 int,
         NMHC int,
         C6H6 float,
         PT08_S2 int,
         NOx int,
         PT08_S3 int,
         NO2 int,
         PT08_S4 int,
         PT08_S5 int,
         T float,
         RH float,
         AH float
         )
        row format delimited
        fields terminated by ','
        tblproperties ("skip.header.line.count"="1");
        
 2. try to place a data into table location
        # Data set loaded in local with help of filezila
        :- /home/cloudera/shekhar/AirQualityUCI.csv
        
 3. Perform a select operation . 
        -select * from airqualityuci_csv limit 10;
        
        :- airqualityuci_csv.date	airqualityuci_csv.time	airqualityuci_csv.co	airqualityuci_csv.pt08_s1	airqualityuci_csv.nmhc	airqualityuci_csv.c6h6	
        airqualityuci_csv.pt08_s2	airqualityuci_csv.nox	airqualityuci_csv.pt08_s3	airqualityuci_csv.no2	airqualityuci_csv.pt08_s4	airqualityuci_csv.pt08_s5
        airqualityuci_csv.t	airqualityuci_csv.rh	airqualityuci_csv.ah
           10-03-04	18:00:00	2.6	1360	150	11.9	1046	166	1056	113	1692	1268	13.6	48.9	0.7578
           10-03-04	19:00:00	2.0	1292	112	9.4	955	103	1174	92	1559	972	13.3	47.7	0.7255
           10-03-04	20:00:00	2.2	1402	88	 9.0	939	131	1140	114	1555	1074	11.9	54.0	0.7502
           10-03-04	21:00:00	2.2	1376	80	 9.2	948	172	1092	122	1584	1203	11.0	60.0	0.7867
           10-03-04	22:00:00	1.6	1272	51	 6.5	836	131	1205	116	1490	1110	11.2	59.6	0.7888
           10-03-04	23:00:00	1.2	1197	38	 4.7	750	89	 1337	96	1393	949	11.2	59.2	0.7848
           11-03-04	0:00:00	 1.2	1185	31	 3.6	690	62	 1462	77	1333	733	11.3	56.8	0.7603
           11-03-04	1:00:00	 1.0	1136	31	 3.3	672	62	 1453	76	1333	730	10.7	60.0	0.7702
           11-03-04	2:00:00	 0.9	1094	24	 2.3	609	45	 1579	60	1276	620	10.7	59.7	0.7648
           11-03-04	3:00:00	 0.6	1010	19	 1.7	561 -200	1705	-200	1235	501	10.3	60.2	0.7517
        
 4. Fetch the result of the select operation in your local as a csv file . 
        - describe formatted airqualityuci_csv;
        :- Location:- hdfs://quickstart.cloudera:8020/user/hive/warehouse/hive_challenge_task3.db/airqualityuci_csv
        
5. Perform group by operation . 
        - select date, sum(co) as Total_co from airqualityuci_csv group by date;
        :- date	total_co
	              date	         total_co
             01-01-05	-150.90000021457672
             01-02-05	68.79999965429306
             01-03-05	24.499999850988388
             01-04-04	61.19999951124191
             01-04-05	-174.5
             01-05-04	-152.70000022649765
             01-06-04	-757.9999999031425
             01-07-04	51.900000393390656
             01-08-04	23.600000113248825
             01-09-04	53.70000022649765
             01-10-04	-1151.3999998569489
             01-11-04	-136.80000007152557
             01-12-04	-126.20000076293945
             02-01-05	49.09999990463257
             02-02-05	80.40000021457672
             02-03-05	-156.79999980330467
             02-04-04	-135.60000032186508
             02-04-05	20.500000178813934
             02-05-04	-163.00000023841858
             02-06-04	-4599.199999988079
             02-07-04	-152.30000019073486
              .
              .
              .
              so on....
        
        - select no2, sum(t) as total_t from airqualityuci_csv group by no2;
        :- no2	     total_t
         -200	23081.799934484065
           2	15.600000381469727
           3	15.5
           5	53.20000076293945
           7	15.399999618530273
           8	35.10000038146973
           9	35.19999885559082
           11	51.69999885559082
           12	47.29999923706055
           13	11.300000190734863
           14	106.89999866485596
           16	47.20000076293945
           17	86.59999966621399
           18	29.90000057220459
           19	68.5
           20	63.39999961853027
               .
               .
               .
               so on....
        
 7. Perform filter operation at least 5 kinds of filter examples . 
	- select sum(rh) from airqualityuci_csv;
	o/p:- 369464.70031929016
	- select max(t) from airqualityuci_csv;
	o/p:- 44.6
	- select min(t) from airqualityuci_csv;
	o/p:- 9.0
        - select avg(ah) from airqualityuci_csv;
	o/p:- 6.837603645078122
	- select count(date) from airqualityuci_csv;
	o/p:- 9471
	
 8. show and example of regex operation
	- describe function regexp_extract;
	o/p:- regexp_extract(str, regexp[, idx])
	- describe function extended regexp_extract;
	o/p:- regexp_extract(str, regexp[, idx])
	  > SELECT regexp_extract('100-200', '(\d+)-(\d+)', 1) FROM src LIMIT 1;
	-  select regexp_extract("Devanshu Shekhar","(D.*)");
	o/p:- Devanshu Shekhar
           Time taken: 0.1 seconds, Fetched: 1 row(s) 
	
9. alter table operation 
        - created a table then alter opertion will be perform-
        -hive> create table airpollution_csv
        > (
        > Date string,
        > Time string,
        > CO float,
        > PT08_S1 int,
        > NMHC int,
        > C6H6 float,
        > PT08_S2 int,
        > NOx int,
        > PT08_S3 int,
        > NO2 int,
        > PT08_S4 int,
        > PT08_S5 int,
        > T float,
        > RH float,
        > AH float
        > )
        > row format delimited
        > fields terminated by ','
        > tblproperties ("skip.header.line.count"="1");
         OK
         Time taken: 13.495 seconds
        
        # alter the table name-
        - hive> alter table airpollution_csv rename to airquality_csv;
          OK
          Time taken: 0.944 seconds
        
10 . drop table operation
        - drop table airpollution;
         OK
         Time taken: 0.264 seconds
        
12 . order by operation . 
        - select date,rh from airqualityuci_csv order by rh;
	:- date   vh
	24-02-05	85.3
	03-12-04	85.3
	22-02-05	85.4
	25-11-04	85.4
	03-12-04	85.5
	09-11-04	85.6
	04-12-04	85.7
	25-11-04	85.7
	09-11-04	85.7
	10-11-04	86.0
	29-10-04	86.5
	04-12-04	86.5
	22-02-05	86.6
	15-01-05	86.6
	04-12-04	87.0
	10-11-04	87.1
	14-09-04	87.2
	04-12-04	88.7
	Time taken: 42.774 seconds, Fetched: 9471 row(s)
        
13 . where clause operations you have to perform .
        
14 . sorting operation you have to perform .
        - select date,t from airqualityuci_csv sort by t limit 10;
	:- date          t
	27-06-04	42.2
	07-07-04	42.5
	07-07-04	42.6
	21-07-04	42.7
	22-07-04	42.8
	23-07-04	42.8
	20-07-04	42.8
	23-07-04	43.1
	22-07-04	43.4
	22-07-04	44.3
	22-07-04	44.6
	Time taken: 45.003 seconds, Fetched: 9471 row(s)
	
15 . distinct operation you have to perform . 
	- select distinct date from airqualityuci_csv;
	o/p:-     date
		01-01-05
		01-02-05
		01-03-05
		01-04-04
		01-04-05
		01-05-04
		01-06-04
		01-07-04
		01-08-04
		  ,
		  ,
		  ,
		so on...
          Time taken: 39.641 seconds, Fetched: 392 row(s)
	
16 . like an operation you have to perform . 
	- hive> create table air_pollution_csv like airqualityuci_csv;
	OK
	Time taken: 0.56 seconds
	
	-hive> select * from air_pollution_csv;
	OK
	air_pollution_csv.date	        air_pollution_csv.time	    air_pollution_csv.co	
	air_pollution_csv.pt08_s1	air_pollution_csv.nmhc	    air_pollution_csv.c6h6	
	air_pollution_csv.pt08_s2	air_pollution_csv.nox	    air_pollution_csv.pt08_s3	
	air_pollution_csv.no2	        air_pollution_csv.pt08_s4   air_pollution_csv.pt08_s5	
	air_pollution_csv.t	        air_pollution_csv.rh	    air_pollution_csv.ah
	Time taken: 0.097 seconds

	- hive> describe air_pollution_csv;
	OK
	col_name	      data_type	     comment
	date                	string              	                    
	time                	string              	                    
	co                  	float               	                    
	pt08_s1             	int                 	                    
	nmhc                	int                 	                    
	c6h6                	float               	                    
	pt08_s2             	int                 	                    
	nox                 	int                 	                    
	pt08_s3             	int                 	                    
	no2                 	int                 	                    
	pt08_s4             	int                 	                    
	pt08_s5             	int                 	                    
	t                   	float               	                    
	rh                  	float               	                    
	ah                  	float

17 . union operation you have to perform . 
	# create two table from airqualityuci_csv table
	-First table-
	- [cloudera@quickstart shekhar]$ cat table1
		1,Amit,Football,23
		2,Devanshu,Cricket,24
		3,Neeraj,Kabady,26
		4,Deepak,Yoga,28

	-hive> create table table_1
	    > (
	    > sr int,
	    > name string,
	    > work string,
	    > age int
	    > )
	    > row format delimited
	    > fields terminated by ',';
	OK
	Time taken: 0.194 seconds
	
	# from like opertion create second table
	-create table table_2 like table_1;
	hive> select * from table_2;
	OK
	1	Shivam	Dance	19
	2	Himanshu	Hockey	28
	3	Chandan	Engg	27
	4	Narayan	Travelling	33
	Time taken: 0.241 seconds, Fetched: 4 row(s)

	# First and second table are join from union operation
	- select * from table_1 union select * from table_2;
	o/p:- 
	1	Shivam	  Dance	        19
	2	Himanshu  Hockey	28
	3	Chandan	  Engg	        27
	4	Narayan	  Travelling	33
	1	Amit	  Football	23
	2	Devanshu  Cricket	24
	3	Neeraj	  Kabady	26
	4	Deepak	  Yoga	        28
	Time taken: 66.579 seconds, Fetched: 8 row(s)

	

18 . table view operation you have to perform . 
       - hive> describe airqualityuci_csv;
          OK
          col_name	data_type	comment
          date                	string              	                    
          time                	string              	                    
          co                  	float               	                    
          pt08_s1             	int                 	                    
          nmhc                	int                 	                    
          c6h6                	float               	                    
          pt08_s2             	int                 	                    
          nox                 	int                 	                    
          pt08_s3             	int                 	                    
          no2                 	int                 	                    
          pt08_s4             	int                 	                    
          pt08_s5             	int                 	                    
          t                   	float               	                    
          rh                  	float               	                    
          ah                  	float 
         Time taken: 0.159 seconds, Fetched: 15 row(s)
